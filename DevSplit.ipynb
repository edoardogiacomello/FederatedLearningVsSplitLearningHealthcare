{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.keras as tk\n",
    "from chexpert_parser import load_dataset\n",
    "# Evita di allocare tutta la memoria video a tensorflow (Chiamare solo al primo import di tf)\n",
    "physical_devices = tf.config.experimental.list_physical_devices('GPU')\n",
    "if len(physical_devices) > 0:\n",
    "    tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "else:\n",
    "    print(\"No GPU found, model running on CPU\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SplitModel():\n",
    "    def __init__(self, configuration, model_folder='./models/'):\n",
    "        ''' \n",
    "            Represents an Entity partecipating in Split Learning (either a Server or a Client).\n",
    "        '''\n",
    "        self.model_folder = model_folder\n",
    "        if configuration['type'] == 'server':\n",
    "            self.is_server = True\n",
    "            self.is_client = False\n",
    "            self.base_server_model = configuration['server_model']\n",
    "            \n",
    "        else:\n",
    "            self.is_server = False\n",
    "            self.is_client = True\n",
    "            self.base_model_bottom = configuration['model_bottom']\n",
    "            self.base_model_top = configuration['model_top']\n",
    "            \n",
    "        \n",
    "        self.split_layer_top = configuration['split_layer_top']\n",
    "        self.split_layer_bottom = configuration['split_layer_bottom']\n",
    "        self.name = configuration['name']\n",
    "        \n",
    "        self.input_shape = configuration['input_shape'] # Shape of input data\n",
    "        self.output_shape = configuration['output_shape'] # Shape of predictions\n",
    "        self.bottom_output_shape = None # Will be computed according to the model\n",
    "        self.top_input_shape = None # Will be computed according to the model\n",
    "        \n",
    "        self.server_model = None # Model that is owned by the server\n",
    "        self.model_bottom = None # Model that receives the data as input\n",
    "        self.model_top = None # Optional: Model that produces the predictions in the U topology\n",
    "        \n",
    "        self.current_epoch = 1\n",
    "        self.current_batch = None # Stores the current batch that is used for training the model\n",
    "        \n",
    "    \n",
    "    \n",
    "    def _cutmodel(self, model_arch, cut_input=None, cut_output=None):\n",
    "        ''' Given a base model (from tf.keras.applications.*) creates a new model \n",
    "            by truncating the original model between cut_input (included) and cut_output (excluded) layers id.\n",
    "            If one of the cuts is None, then the cut will be considered as either the original input or output layer '''\n",
    "        \n",
    "        # FIXME: THIS WORKS ONLY WITH SEQUENTIAL MODELS - WE NEED TO FIND A WAY TO MAKE IT WORK WITH PRETRAINED NETWORKS\n",
    "        base_model = model_arch(input_shape=self.input_shape, include_top=False, weights='imagenet')\n",
    "        #print(\"Cutting {} between {} and {}\".format(str(base_model), cut_input, cut_output))\n",
    "        model = tk.models.Sequential() # TODO: Check if the result is actually the same with all common architectures\n",
    "        for l, layer in enumerate(base_model.layers[cut_input:cut_output]):\n",
    "            try:\n",
    "                model.add(layer)\n",
    "                #print(\"{}:{} OK: {}\".format(l, layer.name, layer.input_shape))\n",
    "            except:\n",
    "                print(\"{}:{} ERROR: {}\".format(l, layer.name, layer.input_shape))\n",
    "        # Calculate the input shape for THIS model. Input layers provide a list of tuples, others provide the tuple. \n",
    "        # Also removing the first \"None\" dimension relative to batch size\n",
    "        input_shape = self.input_shape if cut_input is None or cut_input == 0 else base_model.layers[cut_input].input_shape\n",
    "        model.build(input_shape=input_shape)\n",
    "        return model        \n",
    "    \n",
    "    \n",
    "    def setup(self, load_model='last', seed=1234567890):\n",
    "        '''\n",
    "        Setup the client or server according to the current configuration.\n",
    "        - Loads the architecture(s) and optimizers, according to the provided split layers\n",
    "        - Loads a previous checkpoint (if any)\n",
    "        - Initializes Tensorboard Writers\n",
    "        '''\n",
    "        # Loading and splitting the models\n",
    "        \n",
    "        if self.is_client:\n",
    "            # We are a client. We have a bottom model:\n",
    "            self.model_bottom = self._cutmodel(self.base_model_bottom, cut_input=None, cut_output=self.split_layer_bottom)\n",
    "            \n",
    "            # We can also have a top model:\n",
    "            if self.base_model_top is not None:\n",
    "                self.model_top = self._cutmodel(self.base_model_top, cut_input=self.split_layer_top, cut_output=None)\n",
    "            \n",
    "        if self.is_server:\n",
    "            # We are the server. If the topology is not a U, split_layer_top will be None (since the server will send predictions)\n",
    "            self.server_model = self._cutmodel(self.base_server_model, cut_input=self.split_layer_bottom, cut_output=self.split_layer_top)\n",
    "    \n",
    "        # TODO: Eventually move these into the config dict instead of hardcoding them here.\n",
    "        self.loss_fn = tk.losses.BinaryCrossentropy()\n",
    "        self.optimizer = tk.optimizers.SGD(1e-3)\n",
    "        self.metrics = [tk.metrics.BinaryCrossentropy()]\n",
    "        \n",
    "        # TODO: Initialize tensorboard and model checkpointing\n",
    "        \n",
    "        \n",
    "    def load_datasets(self, dataset_dict):\n",
    "        '''\n",
    "            Loads the datasets according to their presence in the provided dictionary.\n",
    "            Optionally truncates the dataset, if the 'take_only' key is present with a positive value\n",
    "        '''\n",
    "        if 'training' in dataset_dict and dataset_dict['training'] is not None:\n",
    "            take_only = dataset_dict['training']['take_only'] if 'take_only' in dataset_dict['training'] else None\n",
    "            self.training_dataset = load_dataset(dataset_dict['training'], take=take_only)\n",
    "            print(\"{}: Loaded training dataset: {}\".format(self.name, dataset_dict['training']))\n",
    "            \n",
    "        if 'validation' in dataset_dict and dataset_dict['validation'] is not None:\n",
    "            take_only = dataset_dict['validation']['take_only'] if 'take_only' in dataset_dict['validation'] else None\n",
    "            self.validation_dataset = load_dataset(dataset_dict['validation'], take=take_only)\n",
    "            print(\"{}: Loaded validation dataset: {}\".format(self.name, dataset_dict['validation']))\n",
    "            \n",
    "        if 'testing' in dataset_dict and dataset_dict['testing'] is not None:\n",
    "            take_only = dataset_dict['testing']['take_only'] if 'take_only' in dataset_dict['testing'] else None\n",
    "            self.testing_dataset = load_dataset(dataset_dict['testing'], take=take_only)\n",
    "            print(\"{}: Loaded testing dataset: {}\".format(self.name, dataset_dict['testing']))\n",
    "        \n",
    "    \n",
    "    def initiate_training(self):\n",
    "        '''\n",
    "        Initiate a new training round by providing the intermediate layer output for a new batch:\n",
    "        The client privately selects a batch of data and sends the model output to the server\n",
    "        Can only be called on a client.\n",
    "        '''\n",
    "        assert self.is_client, \"Servers cannot initiate training!\"\n",
    "        # TODO: Also implement for validation and testing\n",
    "        # FIXME: THIS MAY RETURN ALWAYS THE SAME BATCH WHEN CALLED (DEPENDING ON TF VERSION) - WE NEED TO CREATE A LAMBDA FOR READING THE DATASET - LIKE IN DCSEG\n",
    "        for step, row in enumerate(self.training_dataset):\n",
    "            self.current_batch = row # We store the current batch for later.. when we have to train our section of the network\n",
    "            return self.model_bottom(row['x'], training=True), row['y']\n",
    "    \n",
    "        \n",
    "    def backpropagation(self, input_=None, gt=None, gradients=None, network_to_train='bottom'):\n",
    "        '''\n",
    "        Perform backpropadation given either an input tensor, a ground truth, or gradients for remote layers (depending on the context).\n",
    "        input_: The input of the network. It's usually received from the server or the client, depending on the topology\n",
    "        gt: It has a value only when the client bottom_model is sending the intermediate output to the server. It's necessary for the server to compute the loss in the I topology\n",
    "        network_to_train: either 'bottom' or 'top', which network to train in the client, for the U topology.\n",
    "        '''\n",
    "        if self.is_server:\n",
    "            # We are receiving intermediate outputs from a client, we need to train the server and return the gradients\n",
    "            with tf.GradientTape() as tape:\n",
    "                predictions = self.server_model(input_, training=True)\n",
    "                loss = self.loss_fn(gt, predictions)\n",
    "            server_gradients = tape.gradient(loss, self.server_model.trainable_weights)\n",
    "            # Train our section of the network\n",
    "            self.optimizer.apply_gradients(zip(server_gradients, self.server_model.trainable_weights))\n",
    "            # TODO: Log training accuracy\n",
    "            return server_gradients\n",
    "        elif self.is_client:\n",
    "            if network_to_train == 'bottom':\n",
    "                pass # TODO: Apply the gradients to bottom network using self.current_batch as target\n",
    "                raise NotImplementedError(\"TODO: Implement backpropagation starting from gradients received by the server\")\n",
    "            elif network_to_train == 'top':\n",
    "                with tf.GradientTape() as tape:\n",
    "                    predictions = self.model_top(input_, training=True)\n",
    "                    loss = self.loss_fn(self.current_batch['y'], predictions)\n",
    "                server_gradients = tape.gradient(loss, self.server_model.trainable_weights)\n",
    "        \n",
    "    def forward_pass(self, intermediate_output_tensor)\n",
    "        if self.is_server:\n",
    "            # We are receiving intermediate outputs from a client, we need to provide the model output\n",
    "            return self.server_model(intermediate_output_tensor, training=True)\n",
    "        elif self.is_client:\n",
    "            # We received an intermediate_output from the server for calculating predictions\n",
    "            predictions = self.model_top(intermediate_output_tensor, training=False)\n",
    "            # TODO: Do something with the predictions\n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SplitTraining():\n",
    "    def __init__(self, configuration, topology, model_folder='./models/'):\n",
    "        ''' Initialize the Server/Clients given a Server/Client definition and a topology. Orchestrates the training process and the aggregation according to the topology'''\n",
    "        self.model_folder = model_folder\n",
    "        self.configuration = configuration\n",
    "        self.topology = topology\n",
    "        self.server = None\n",
    "        self.clients = dict()\n",
    "        \n",
    "    def setup(self):\n",
    "        # Creating the server and the clients\n",
    "        for config in self.configuration:\n",
    "            if config['type'] == 'server':\n",
    "                self.server = SplitModel(configuration=config, model_folder=self.model_folder)\n",
    "            elif config['type'] == 'client':\n",
    "                self.clients[config['name']] = SplitModel(configuration=config, model_folder=self.model_folder)\n",
    "        print(\"Created {} server and {} clients\".format(len([self.server]), len(self.clients)))\n",
    "        \n",
    "        print(\"Building the models...\")\n",
    "        self.server.setup()\n",
    "        for c_name, client in self.clients.items():\n",
    "            print(\"Building {}...\".format(c_name))\n",
    "            client.setup()\n",
    "            \n",
    "    def print_config(self):\n",
    "        print(\"Server Model:\")\n",
    "        self.server.server_model.summary()\n",
    "        for c_name, client in self.clients.items():\n",
    "            print(\"{}: Bottom Model\".format(c_name))\n",
    "            client.model_bottom.summary()\n",
    "            if client.model_top is not None:\n",
    "                print(\"{}: Top Model\".format(c_name))\n",
    "                client.model_top.summary()\n",
    "        \n",
    "    def load_datasets(self, datasets_dict):\n",
    "        for client_name, datasets in datasets_dict.items():\n",
    "            self.clients[client_name].load_datasets(datasets)\n",
    "    \n",
    "    def split_training(self):\n",
    "        \n",
    "        # TODO: Eventually define more complex strategies, like:\n",
    "        # Client selection, Output/Gradient aggregation, etc.\n",
    "        # For the moment we train one client at a time\n",
    "        \n",
    "        for current_client_name, current_client in self.clients.items():\n",
    "            # The client provides its intermediate output\n",
    "            client_intermediate_output, client_gt = current_client.initiate_training()\n",
    "            print(\"Client {} sending a tensor with shape: {}\".format(current_client_name, client_intermediate_output.shape))\n",
    "            \n",
    "            if self.topology == 'I':\n",
    "                # Sending the intermediate output to the server, which trains its section of the network and return its gradient\n",
    "                server_gradient = self.server.backpropagation(input_=client_intermediate_output, gt=client_gt, network='server')\n",
    "                # Sending the server gradient back to the client, which trains its section of the network\n",
    "                current_client.backpropagation(gradients=server_gradient, network='bottom')\n",
    "                \n",
    "            elif self.topology == 'U':\n",
    "                # Sending the intermediate output to the server, which returns its intermediate output\n",
    "                server_intermediate_output = self.server.forward_pass(client_intermediate_output)\n",
    "                # Sending the server output to the client's top model to compute the gradient\n",
    "                client_top_gradient = current_client.backpropagation(input_=server_gradient, network='top')\n",
    "                # Use the gradient to train the server model, obtaining the server gradient\n",
    "                server_gradient = self.server.backpropagation(gradients=client_top_gradient, network='server')\n",
    "                # Use the server gradient to train the client bottom model\n",
    "                current_client.backpropagation(gradients=server_gradient, network='bottom')\n",
    "            \n",
    "            elif self.topology == 'V':\n",
    "                # TODO: Implement the aggregation for V topology\n",
    "                raise NotImplementedError(\"V Topology not implemented yet\")\n",
    "            # Sending the intermediate output to the server, which returns its intermediate output or predictions\n",
    "            server_intermediate_output = self.server.on_forwardpass_received(intermediate_output, phase='training')\n",
    "            \n",
    "            # Sending the output back to the client\n",
    "            current_client = on_forwardpass_received(intermediate_output, phase='training')\n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DummyModel(tk.Model):\n",
    "    ''' THIS IS A PLACEHOLDER THAT CREATES A DUMMY MODEL UNTIL WE FIND A WAY TO SPLIT PRE-TRAINED MODELS CORRECTLY'''\n",
    "    def __init__(self, input_shape,  include_top=False, weights='imagenet'):\n",
    "        input_layer = tk.layers.Input(shape=input_shape)\n",
    "        \n",
    "        latest_layer = input_layer\n",
    "        for i in range(3):\n",
    "            # Define a block \n",
    "            conv = tk.layers.Conv2D(3, 3, activation='relu', name='block_{}_conv'.format(i))(latest_layer)\n",
    "            bn = tk.layers.BatchNormalization(name='block_{}_bn'.format(i))(conv)\n",
    "            latest_layer=bn\n",
    "        gap = tk.layers.GlobalAveragePooling2D()(latest_layer)\n",
    "        predictions = tk.layers.Dense(14, activation='sigmoid')(gap)\n",
    "        return super().__init__(inputs=input_layer, outputs=predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing\n",
    "\n",
    "from tensorflow.keras.applications.mobilenet_v2 import MobileNetV2\n",
    "from tensorflow.keras.applications.densenet import DenseNet121\n",
    "\n",
    "# Definitions:\n",
    "# Model Bottom: It's the model of the client that receives data as input.\n",
    "# Model Top: It's the last part of the network (the ones that gives predictions). It is missing in the I an V topologies as it's part of the server model.\n",
    "\n",
    "# Topology (from https://arxiv.org/pdf/1812.00564.pdf):\n",
    "# I: Vanilla Split Learning - It is implemented as a U model with a missing top (i.e. predictions are always sent to the client that made the request.)\n",
    "# U: SL Without Label Sharing\n",
    "# V: SL for Vertically Partitioned Data\n",
    "BASE_DATASET_PATH = './datasets/CheXpertFederated/'\n",
    "# Test with Densenet (53, 313)\n",
    "SL_BOTTOM = 3\n",
    "SL_TOP = None\n",
    "input_shape = (224, 224, 3) # Shape of the input of client models (data shape). Can be different for each client.\n",
    "output_shape = (14,) # Shape of the outputs. Can be different for each client when using the U topology.\n",
    "client_model_top = None\n",
    "client_model_bottom = DummyModel\n",
    "server_model = DummyModel\n",
    "\n",
    "\n",
    "configuration = [   \n",
    "                    {'type':'server', 'name':'server', 'server_model':server_model, 'split_layer_top':SL_TOP, 'split_layer_bottom':SL_BOTTOM, 'input_shape':input_shape, 'output_shape':output_shape},\n",
    "                    {'type':'client', 'name':'client_1', 'model_bottom': client_model_bottom, 'split_layer_bottom': SL_BOTTOM, 'model_top': client_model_top, 'split_layer_top':SL_TOP, 'input_shape':input_shape, 'output_shape':output_shape},\n",
    "                    {'type':'client', 'name':'client_2', 'model_bottom': client_model_bottom, 'split_layer_bottom': SL_BOTTOM, 'model_top': client_model_top, 'split_layer_top':SL_TOP, 'input_shape':input_shape, 'output_shape':output_shape},\n",
    "                    {'type':'client', 'name':'client_3', 'model_bottom': client_model_bottom, 'split_layer_bottom': SL_BOTTOM, 'model_top': client_model_top, 'split_layer_top':SL_TOP, 'input_shape':input_shape, 'output_shape':output_shape}\n",
    "                ]\n",
    "\n",
    "split_datasets = {\n",
    "                    'client_1' : {'training': BASE_DATASET_PATH + 'train_def-part-0.tfrecord', 'validation': None, 'testing': None},\n",
    "                    'client_2' : {'training': BASE_DATASET_PATH + 'train_def-part-1.tfrecord', 'validation': None, 'testing': None},\n",
    "                    'client_3' : {'training': BASE_DATASET_PATH + 'train_def-part-2.tfrecord', 'validation': None, 'testing': None},\n",
    "                 }\n",
    "\n",
    "splitprocess = SplitTraining(configuration, topology='I')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created 1 server and 3 clients\n",
      "Building the models...\n",
      "Building client_1...\n",
      "Building client_2...\n",
      "Building client_3...\n",
      "client_1: Loaded training dataset: ./datasets/CheXpertFederated/train_def-part-0.tfrecord\n",
      "client_2: Loaded training dataset: ./datasets/CheXpertFederated/train_def-part-1.tfrecord\n",
      "client_3: Loaded training dataset: ./datasets/CheXpertFederated/train_def-part-2.tfrecord\n"
     ]
    }
   ],
   "source": [
    "splitprocess.setup()\n",
    "# splitprocess.print_config()\n",
    "splitprocess.load_datasets(split_datasets)\n",
    "splitprocess.split_training()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
