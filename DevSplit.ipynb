{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.9"
    },
    "colab": {
      "name": "DevSplit.ipynb",
      "provenance": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eriuPaTnxepc",
        "outputId": "4e847ae4-161a-430b-b72c-833e3d602220"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T5g55CGbVAnP"
      },
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow.keras as tk\n",
        "from chexpert_parser import load_dataset\n",
        "# Evita di allocare tutta la memoria video a tensorflow (Chiamare solo al primo import di tf)\n",
        "physical_devices = tf.config.experimental.list_physical_devices('GPU')\n",
        "if len(physical_devices) > 0:\n",
        "    tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
        "else:\n",
        "    print(\"No GPU found, model running on CPU\")\n"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oAPyLr0BVAnQ"
      },
      "source": [
        "class SplitModel():\n",
        "    def __init__(self, configuration, model_folder='./models/'):\n",
        "        ''' \n",
        "            Represents an Entity partecipating in Split Learning (either a Server or a Client).\n",
        "        '''\n",
        "        self.model_folder = model_folder\n",
        "        if configuration['type'] == 'server':\n",
        "            self.is_server = True\n",
        "            self.is_client = False\n",
        "            self.base_server_model = configuration['server_model']\n",
        "            \n",
        "        else:\n",
        "            self.is_server = False\n",
        "            self.is_client = True\n",
        "            self.base_model_bottom = configuration['model_bottom']\n",
        "            self.base_model_top = configuration['model_top']\n",
        "            self.client_iterator_flag = False\n",
        "            \n",
        "        \n",
        "        self.split_layer_top = configuration['split_layer_top']\n",
        "        self.split_layer_bottom = configuration['split_layer_bottom']\n",
        "        self.name = configuration['name']\n",
        "        \n",
        "        self.input_shape = configuration['input_shape'] # Shape of input data\n",
        "        self.output_shape = configuration['output_shape'] # Shape of predictions\n",
        "        self.bottom_output_shape = None # Will be computed according to the model\n",
        "        self.top_input_shape = None # Will be computed according to the model\n",
        "        \n",
        "        self.server_model = None # Model that is owned by the server\n",
        "        self.model_bottom = None # Model that receives the data as input\n",
        "        self.model_top = None # Optional: Model that produces the predictions in the U topology\n",
        "        \n",
        "        self.current_epoch = 1\n",
        "        self.current_batch = None # Stores the current batch that is used for training the model\n",
        "        \n",
        "    \n",
        "    \n",
        "    def _cutmodel(self, model_arch, cut_input=None, cut_output=None):\n",
        "        ''' Given a base model (from tf.keras.applications.*) creates a new model \n",
        "            by truncating the original model between cut_input (included) and cut_output (excluded) layers id.\n",
        "            If one of the cuts is None, then the cut will be considered as either the original input or output layer '''\n",
        "        \n",
        "        # FIXME: THIS WORKS ONLY WITH SEQUENTIAL MODELS - WE NEED TO FIND A WAY TO MAKE IT WORK WITH PRETRAINED NETWORKS\n",
        "        base_model = model_arch(input_shape=self.input_shape, include_top=False, weights='imagenet')\n",
        "        #print(\"Cutting {} between {} and {}\".format(str(base_model), cut_input, cut_output))\n",
        "        model = tk.models.Sequential() # TODO: Check if the result is actually the same with all common architectures\n",
        "        for l, layer in enumerate(base_model.layers[cut_input:cut_output]):\n",
        "            try:\n",
        "                model.add(layer)\n",
        "                #print(\"{}:{} OK: {}\".format(l, layer.name, layer.input_shape))\n",
        "            except:\n",
        "                print(\"{}:{} ERROR: {}\".format(l, layer.name, layer.input_shape))\n",
        "        # Calculate the input shape for THIS model. Input layers provide a list of tuples, others provide the tuple. \n",
        "        # Also removing the first \"None\" dimension relative to batch size\n",
        "        input_shape = self.input_shape if cut_input is None or cut_input == 0 else base_model.layers[cut_input].input_shape\n",
        "        model.build(input_shape=input_shape)\n",
        "        return model        \n",
        "    \n",
        "    \n",
        "    def setup(self, load_model='last', seed=1234567890):\n",
        "        '''\n",
        "        Setup the client or server according to the current configuration.\n",
        "        - Loads the architecture(s) and optimizers, according to the provided split layers\n",
        "        - Loads a previous checkpoint (if any)\n",
        "        - Initializes Tensorboard Writers\n",
        "        '''\n",
        "        # Loading and splitting the models\n",
        "        \n",
        "        if self.is_client:\n",
        "            # We are a client. We have a bottom model:\n",
        "            self.model_bottom = self._cutmodel(self.base_model_bottom, cut_input=None, cut_output=self.split_layer_bottom)\n",
        "            \n",
        "            # We can also have a top model:\n",
        "            if self.base_model_top is not None:\n",
        "                self.model_top = self._cutmodel(self.base_model_top, cut_input=self.split_layer_top, cut_output=None)\n",
        "            \n",
        "        if self.is_server:\n",
        "            # We are the server. If the topology is not a U, split_layer_top will be None (since the server will send predictions)\n",
        "            self.server_model = self._cutmodel(self.base_server_model, cut_input=self.split_layer_bottom, cut_output=self.split_layer_top)\n",
        "    \n",
        "        # TODO: Eventually move these into the config dict instead of hardcoding them here.\n",
        "        self.loss_fn = tk.losses.BinaryCrossentropy()\n",
        "        self.optimizer = tk.optimizers.SGD(1e-2)\n",
        "        self.metrics = [tk.metrics.BinaryCrossentropy()]\n",
        "        \n",
        "        # TODO: Initialize tensorboard and model checkpointing\n",
        "        \n",
        "        \n",
        "    def load_datasets(self, dataset_dict):\n",
        "        '''\n",
        "            Loads the datasets according to their presence in the provided dictionary.\n",
        "            Optionally truncates the dataset, if the 'take_only' key is present with a positive value\n",
        "        '''\n",
        "        if 'training' in dataset_dict and dataset_dict['training'] is not None:\n",
        "            take_only = dataset_dict['training']['take_only'] if 'take_only' in dataset_dict['training'] else None\n",
        "            self.training_dataset = load_dataset(dataset_dict['training'], take=take_only)\n",
        "            print(\"{}: Loaded training dataset: {}\".format(self.name, dataset_dict['training']))\n",
        "            \n",
        "        if 'validation' in dataset_dict and dataset_dict['validation'] is not None:\n",
        "            take_only = dataset_dict['validation']['take_only'] if 'take_only' in dataset_dict['validation'] else None\n",
        "            self.validation_dataset = load_dataset(dataset_dict['validation'], take=take_only)\n",
        "            print(\"{}: Loaded validation dataset: {}\".format(self.name, dataset_dict['validation']))\n",
        "            \n",
        "        if 'testing' in dataset_dict and dataset_dict['testing'] is not None:\n",
        "            take_only = dataset_dict['testing']['take_only'] if 'take_only' in dataset_dict['testing'] else None\n",
        "            self.testing_dataset = load_dataset(dataset_dict['testing'], take=take_only)\n",
        "            print(\"{}: Loaded testing dataset: {}\".format(self.name, dataset_dict['testing']))\n",
        "    \n",
        "    def initiate_training(self):\n",
        "        '''\n",
        "        Initiate a new training round by providing the intermediate layer output for a new batch:\n",
        "        The client privately selects a batch of data and sends the model output to the server\n",
        "        Can only be called on a client.\n",
        "        '''\n",
        "        assert self.is_client, \"Servers cannot initiate training!\"\n",
        "        # TODO: Also implement for validation and testing\n",
        "        # FIXME: THIS MAY RETURN ALWAYS THE SAME BATCH WHEN CALLED (DEPENDING ON TF VERSION) - WE NEED TO CREATE A LAMBDA FOR READING THE DATASET - LIKE IN DCSEG\n",
        "        try:\n",
        "            row = next(self.my_iterator)\n",
        "            self.current_batch = row # We store the current batch for later.. when we have to train our section of the network\n",
        "            return self.model_bottom(row['x'], training=True), row['y']\n",
        "        except StopIteration:\n",
        "            print(\"****TROVATA LA FINE*****\")\n",
        "            self.client_iterator_flag = False\n",
        "            return False, False\n",
        "\n",
        "\n",
        "    def backpropagation(self, input_=None, gt=None, gradients=None, network_to_train='bottom', conf='I'):\n",
        "        '''\n",
        "        Perform backpropagation given either an input tensor, a ground truth, or gradients for remote layers (depending on the context).\n",
        "        input_: The input of the network. It's usually received from the server or the client, depending on the topology\n",
        "        gt: It has a value only when the client bottom_model is sending the intermediate output to the server. It's necessary for the server to compute the loss in the I topology\n",
        "        network_to_train: either 'bottom' or 'top', which network to train in the client, for the U topology.\n",
        "        '''\n",
        "        if self.is_server:\n",
        "            if conf == 'I':\n",
        "                # We are receiving intermediate outputs from a client, we need to train the server and return the gradients\n",
        "                with tf.GradientTape() as tape:\n",
        "                    predictions = self.server_model(input_, training=True)\n",
        "                    loss = self.loss_fn(gt, predictions)\n",
        "                server_gradients = tape.gradient(loss, self.server_model.trainable_weights)\n",
        "                # Train our section of the network\n",
        "                self.optimizer.apply_gradients(zip(server_gradients, self.server_model.trainable_weights))\n",
        "                # TODO: Log training accuracy\n",
        "                return server_gradients, loss\n",
        "            elif conf == 'U':\n",
        "                self.optimizer.apply_gradients(zip(gradients, self.server_model.trainable_weights))\n",
        "        elif self.is_client:\n",
        "            if network_to_train == 'bottom':\n",
        "                self.optimizer.apply_gradients(zip(gradients, self.model_bottom.trainable_weights))\n",
        "            elif network_to_train == 'top':\n",
        "                with tf.GradientTape() as tape:\n",
        "                    predictions = self.model_top(input_, training=True)\n",
        "                    loss = self.loss_fn(self.current_batch['y'], predictions)\n",
        "                grads = tape.gradient(loss, self.model_top.trainable_weights)\n",
        "                self.optimizer.apply_gradients(zip(grads, self.model_top.trainable_weights))\n",
        "                return grads, loss\n",
        "        \n",
        "    def forward_pass(self, intermediate_output_tensor):\n",
        "        if self.is_server:\n",
        "            # We are receiving intermediate outputs from a client, we need to provide the model output\n",
        "            return self.server_model(intermediate_output_tensor, training=True)\n",
        "        elif self.is_client:\n",
        "            # We received an intermediate_output from the server for calculating predictions\n",
        "            predictions = self.model_top(intermediate_output_tensor, training=False)\n",
        "            # TODO: Do something with the predictions\n",
        "\n",
        "    def create_iterator(self):\n",
        "        self.my_iterator = iter(self.training_dataset)\n",
        "        self.client_iterator_flag = True # Setting True the flag of iterator. Flag will be False when end of iterator is found \n"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ueIQ0WGsVAnQ"
      },
      "source": [
        "class SplitTraining():\n",
        "    def __init__(self, configuration, topology, model_folder='./models/'):\n",
        "        ''' Initialize the Server/Clients given a Server/Client definition and a topology. Orchestrates the training process and the aggregation according to the topology'''\n",
        "        self.model_folder = model_folder\n",
        "        self.configuration = configuration\n",
        "        self.topology = topology\n",
        "        self.server = None\n",
        "        self.clients = dict()\n",
        "        \n",
        "    def setup(self):\n",
        "        # Creating the server and the clients\n",
        "        for config in self.configuration:\n",
        "            if config['type'] == 'server':\n",
        "                self.server = SplitModel(configuration=config, model_folder=self.model_folder)\n",
        "            elif config['type'] == 'client':\n",
        "                self.clients[config['name']] = SplitModel(configuration=config, model_folder=self.model_folder)\n",
        "        print(\"Created {} server and {} clients\".format(len([self.server]), len(self.clients)))\n",
        "        \n",
        "        print(\"Building the models...\")\n",
        "        self.server.setup()\n",
        "        for c_name, client in self.clients.items():\n",
        "            print(\"Building {}...\".format(c_name))\n",
        "            client.setup()\n",
        "            \n",
        "    def print_config(self):\n",
        "        print(\"Server Model:\")\n",
        "        self.server.server_model.summary()\n",
        "        for c_name, client in self.clients.items():\n",
        "            print(\"{}: Bottom Model\".format(c_name))\n",
        "            client.model_bottom.summary()\n",
        "            if client.model_top is not None:\n",
        "                print(\"{}: Top Model\".format(c_name))\n",
        "                client.model_top.summary()\n",
        "        \n",
        "    def load_datasets(self, datasets_dict):\n",
        "        for client_name, datasets in datasets_dict.items():\n",
        "            self.clients[client_name].load_datasets(datasets)\n",
        "    \n",
        "    def split_training(self):\n",
        "        \n",
        "        # TODO: Eventually define more complex strategies, like:\n",
        "        # Client selection, Output/Gradient aggregation, etc.\n",
        "        for current_client_name, current_client in self.clients.items():\n",
        "            for epoch in range(3):\n",
        "                print(\"\\nStart of epoch %d\" % (epoch))\n",
        "                current_client.create_iterator()\n",
        "                step = 0 \n",
        "                while current_client.client_iterator_flag is True :\n",
        "                    # The client provides its intermediate output\n",
        "                    client_intermediate_output, client_gt = current_client.initiate_training()\n",
        "                    if current_client.client_iterator_flag == False:    # When flag is false, end of iterator is found. We don't have any intermediate output.\n",
        "                        break\n",
        "                    step += 1               \n",
        "                    if self.topology == 'I':\n",
        "                        # Sending the intermediate output to the server, which trains its section of the network and return its gradient\n",
        "                        server_gradient, train_loss = self.server.backpropagation(input_=client_intermediate_output, gt=client_gt, network_to_train='server')\n",
        "                        # Sending the server gradient back to the client, which trains its section of the network\n",
        "                        current_client.backpropagation(gradients=server_gradient, network_to_train='bottom')\n",
        "                    \n",
        "                    elif self.topology == 'U':\n",
        "                        # Sending the intermediate output to the server, which returns its intermediate output\n",
        "                        server_intermediate_output = self.server.forward_pass(client_intermediate_output)\n",
        "                        # Sending the server output to the client's top model to compute the gradient\n",
        "                        client_top_gradient, train_loss = current_client.backpropagation(input_=server_intermediate_output, network_to_train='top')\n",
        "                        # Use the gradient to train the server model, obtaining the server gradient\n",
        "                        self.server.backpropagation(gradients=client_top_gradient, network_to_train='server', conf='U')\n",
        "                        # Use the server gradient to train the client bottom model\n",
        "                        current_client.backpropagation(gradients=client_top_gradient, network_to_train='bottom')\n",
        "                    \n",
        "                    elif self.topology == 'V':\n",
        "                        # TODO: Implement the aggregation for V topology\n",
        "                        raise NotImplementedError(\"V Topology not implemented yet\")\n",
        "\n",
        "                    if step % 10 == 0:\n",
        "                        print(\"Training loss (for one batch) at step %d: %.4f\" % (step, float(train_loss)) )\n",
        "                        print(\"Seen so far: %d samples\" % ((step + 1) * 32))\n",
        "\n",
        "\n",
        "                    # Sending the intermediate output to the server, which returns its intermediate output or predictions\n",
        "                    ###################################server_intermediate_output = self.server.on_forwardpass_received(intermediate_output, phase='training')\n",
        "                    \n",
        "                    # Sending the output back to the client\n",
        "                    ###################################current_client = on_forwardpass_received(intermediate_output, phase='training')"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bQcU9gx-VAnQ"
      },
      "source": [
        "class DummyModel(tk.Model):\n",
        "    ''' THIS IS A PLACEHOLDER THAT CREATES A DUMMY MODEL UNTIL WE FIND A WAY TO SPLIT PRE-TRAINED MODELS CORRECTLY'''\n",
        "    def __init__(self, input_shape,  include_top=False, weights='imagenet'):\n",
        "        input_layer = tk.layers.Input(shape=input_shape)\n",
        "        \n",
        "        latest_layer = input_layer\n",
        "        for i in range(3):\n",
        "            # Define a block \n",
        "            conv = tk.layers.Conv2D(3, 3, activation='relu', name='block_{}_conv'.format(i))(latest_layer)\n",
        "            bn = tk.layers.BatchNormalization(name='block_{}_bn'.format(i))(conv)\n",
        "            latest_layer=bn\n",
        "        gap = tk.layers.GlobalAveragePooling2D()(latest_layer)\n",
        "        predictions = tk.layers.Dense(14, activation='sigmoid')(gap)\n",
        "        return super().__init__(inputs=input_layer, outputs=predictions)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tJO5QBYNVAnQ"
      },
      "source": [
        "# Testing\n",
        "\n",
        "from tensorflow.keras.applications.mobilenet_v2 import MobileNetV2\n",
        "from tensorflow.keras.applications.densenet import DenseNet121\n",
        "\n",
        "# Definitions:\n",
        "# Model Bottom: It's the model of the client that receives data as input.\n",
        "# Model Top: It's the last part of the network (the ones that gives predictions). It is missing in the I an V topologies as it's part of the server model.\n",
        "\n",
        "# Topology (from https://arxiv.org/pdf/1812.00564.pdf):\n",
        "# I: Vanilla Split Learning - It is implemented as a U model with a missing top (i.e. predictions are always sent to the client that made the request.)\n",
        "# U: SL Without Label Sharing\n",
        "# V: SL for Vertically Partitioned Data\n",
        "BASE_DATASET_PATH = '/content/drive/MyDrive/tfrecords/'\n",
        "# Test with Densenet (53, 313)\n",
        "SL_BOTTOM = 3\n",
        "SL_TOP = 5\n",
        "input_shape = (224, 224, 3) # Shape of the input of client models (data shape). Can be different for each client.\n",
        "output_shape = (14,) # Shape of the outputs. Can be different for each client when using the U topology.\n",
        "client_model_top = DummyModel\n",
        "client_model_bottom = DummyModel\n",
        "server_model = DummyModel\n",
        "\n",
        "\n",
        "configuration = [   \n",
        "                    {'type':'server', 'name':'server', 'server_model':server_model, 'split_layer_top':SL_TOP, 'split_layer_bottom':SL_BOTTOM, 'input_shape':input_shape, 'output_shape':output_shape},\n",
        "                    {'type':'client', 'name':'client_1', 'model_bottom': client_model_bottom, 'split_layer_bottom': SL_BOTTOM, 'model_top': client_model_top, 'split_layer_top':SL_TOP, 'input_shape':input_shape, 'output_shape':output_shape},\n",
        "                    {'type':'client', 'name':'client_2', 'model_bottom': client_model_bottom, 'split_layer_bottom': SL_BOTTOM, 'model_top': client_model_top, 'split_layer_top':SL_TOP, 'input_shape':input_shape, 'output_shape':output_shape},\n",
        "                    {'type':'client', 'name':'client_3', 'model_bottom': client_model_bottom, 'split_layer_bottom': SL_BOTTOM, 'model_top': client_model_top, 'split_layer_top':SL_TOP, 'input_shape':input_shape, 'output_shape':output_shape}\n",
        "                ]\n",
        "\n",
        "split_datasets = {\n",
        "                    'client_1' : {'training': BASE_DATASET_PATH + 'train_6000-part-0.tfrecord', 'validation': None, 'testing': None},\n",
        "                    'client_2' : {'training': BASE_DATASET_PATH + 'train_6000-part-1.tfrecord', 'validation': None, 'testing': None},\n",
        "                    'client_3' : {'training': BASE_DATASET_PATH + 'train_6000-part-2.tfrecord', 'validation': None, 'testing': None},\n",
        "                 }\n",
        "\n",
        "splitprocess = SplitTraining(configuration, topology='U')\n"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gWY2-AL8Srd9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "82b1b665-5bff-439d-d456-2bbde0d26584"
      },
      "source": [
        "splitprocess.setup()\r\n",
        "# splitprocess.print_config()\r\n",
        "splitprocess.load_datasets(split_datasets)\r\n",
        "splitprocess.split_training()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Created 1 server and 3 clients\n",
            "Building the models...\n",
            "Building client_1...\n",
            "Building client_2...\n",
            "Building client_3...\n",
            "client_1: Loaded training dataset: /content/drive/MyDrive/tfrecords/train_6000-part-0.tfrecord\n",
            "client_2: Loaded training dataset: /content/drive/MyDrive/tfrecords/train_6000-part-1.tfrecord\n",
            "client_3: Loaded training dataset: /content/drive/MyDrive/tfrecords/train_6000-part-2.tfrecord\n",
            "\n",
            "Start of epoch 0\n",
            "Training loss (for one batch) at step 10: 0.6972\n",
            "Seen so far: 352 samples\n",
            "Training loss (for one batch) at step 20: 0.6932\n",
            "Seen so far: 672 samples\n",
            "Training loss (for one batch) at step 30: 0.6921\n",
            "Seen so far: 992 samples\n",
            "Training loss (for one batch) at step 40: 0.6910\n",
            "Seen so far: 1312 samples\n",
            "Training loss (for one batch) at step 50: 0.6914\n",
            "Seen so far: 1632 samples\n",
            "Training loss (for one batch) at step 60: 0.6906\n",
            "Seen so far: 1952 samples\n",
            "****TROVATA LA FINE*****\n",
            "\n",
            "Start of epoch 1\n",
            "Training loss (for one batch) at step 10: 0.6886\n",
            "Seen so far: 352 samples\n",
            "Training loss (for one batch) at step 20: 0.6833\n",
            "Seen so far: 672 samples\n",
            "Training loss (for one batch) at step 30: 0.6856\n",
            "Seen so far: 992 samples\n",
            "Training loss (for one batch) at step 40: 0.6860\n",
            "Seen so far: 1312 samples\n",
            "Training loss (for one batch) at step 50: 0.6835\n",
            "Seen so far: 1632 samples\n",
            "Training loss (for one batch) at step 60: 0.6831\n",
            "Seen so far: 1952 samples\n",
            "****TROVATA LA FINE*****\n",
            "\n",
            "Start of epoch 2\n",
            "Training loss (for one batch) at step 10: 0.6820\n",
            "Seen so far: 352 samples\n",
            "Training loss (for one batch) at step 20: 0.6766\n",
            "Seen so far: 672 samples\n",
            "Training loss (for one batch) at step 30: 0.6807\n",
            "Seen so far: 992 samples\n",
            "Training loss (for one batch) at step 40: 0.6779\n",
            "Seen so far: 1312 samples\n",
            "Training loss (for one batch) at step 50: 0.6782\n",
            "Seen so far: 1632 samples\n",
            "Training loss (for one batch) at step 60: 0.6782\n",
            "Seen so far: 1952 samples\n",
            "****TROVATA LA FINE*****\n",
            "\n",
            "Start of epoch 0\n",
            "Training loss (for one batch) at step 10: 0.6936\n",
            "Seen so far: 352 samples\n",
            "Training loss (for one batch) at step 20: 0.6920\n",
            "Seen so far: 672 samples\n",
            "Training loss (for one batch) at step 30: 0.6922\n",
            "Seen so far: 992 samples\n",
            "Training loss (for one batch) at step 40: 0.6915\n",
            "Seen so far: 1312 samples\n",
            "Training loss (for one batch) at step 50: 0.6877\n",
            "Seen so far: 1632 samples\n",
            "Training loss (for one batch) at step 60: 0.6874\n",
            "Seen so far: 1952 samples\n",
            "****TROVATA LA FINE*****\n",
            "\n",
            "Start of epoch 1\n",
            "Training loss (for one batch) at step 10: 0.6876\n",
            "Seen so far: 352 samples\n",
            "Training loss (for one batch) at step 20: 0.6879\n",
            "Seen so far: 672 samples\n",
            "Training loss (for one batch) at step 30: 0.6849\n",
            "Seen so far: 992 samples\n",
            "Training loss (for one batch) at step 40: 0.6840\n",
            "Seen so far: 1312 samples\n",
            "Training loss (for one batch) at step 50: 0.6813\n",
            "Seen so far: 1632 samples\n",
            "Training loss (for one batch) at step 60: 0.6793\n",
            "Seen so far: 1952 samples\n",
            "****TROVATA LA FINE*****\n",
            "\n",
            "Start of epoch 2\n",
            "Training loss (for one batch) at step 10: 0.6800\n",
            "Seen so far: 352 samples\n",
            "Training loss (for one batch) at step 20: 0.6775\n",
            "Seen so far: 672 samples\n",
            "Training loss (for one batch) at step 30: 0.6761\n",
            "Seen so far: 992 samples\n",
            "Training loss (for one batch) at step 40: 0.6754\n",
            "Seen so far: 1312 samples\n",
            "Training loss (for one batch) at step 50: 0.6745\n",
            "Seen so far: 1632 samples\n",
            "Training loss (for one batch) at step 60: 0.6737\n",
            "Seen so far: 1952 samples\n",
            "****TROVATA LA FINE*****\n",
            "\n",
            "Start of epoch 0\n",
            "Training loss (for one batch) at step 10: 0.6941\n",
            "Seen so far: 352 samples\n",
            "Training loss (for one batch) at step 20: 0.6935\n",
            "Seen so far: 672 samples\n",
            "Training loss (for one batch) at step 30: 0.6939\n",
            "Seen so far: 992 samples\n",
            "Training loss (for one batch) at step 40: 0.6883\n",
            "Seen so far: 1312 samples\n",
            "Training loss (for one batch) at step 50: 0.6901\n",
            "Seen so far: 1632 samples\n",
            "Training loss (for one batch) at step 60: 0.6889\n",
            "Seen so far: 1952 samples\n",
            "****TROVATA LA FINE*****\n",
            "\n",
            "Start of epoch 1\n",
            "Training loss (for one batch) at step 10: 0.6899\n",
            "Seen so far: 352 samples\n",
            "Training loss (for one batch) at step 20: 0.6870\n",
            "Seen so far: 672 samples\n",
            "Training loss (for one batch) at step 30: 0.6876\n",
            "Seen so far: 992 samples\n",
            "Training loss (for one batch) at step 40: 0.6852\n",
            "Seen so far: 1312 samples\n",
            "Training loss (for one batch) at step 50: 0.6830\n",
            "Seen so far: 1632 samples\n",
            "Training loss (for one batch) at step 60: 0.6823\n",
            "Seen so far: 1952 samples\n",
            "****TROVATA LA FINE*****\n",
            "\n",
            "Start of epoch 2\n",
            "Training loss (for one batch) at step 10: 0.6830\n",
            "Seen so far: 352 samples\n",
            "Training loss (for one batch) at step 20: 0.6815\n",
            "Seen so far: 672 samples\n",
            "Training loss (for one batch) at step 30: 0.6846\n",
            "Seen so far: 992 samples\n",
            "Training loss (for one batch) at step 40: 0.6790\n",
            "Seen so far: 1312 samples\n",
            "Training loss (for one batch) at step 50: 0.6769\n",
            "Seen so far: 1632 samples\n",
            "Training loss (for one batch) at step 60: 0.6742\n",
            "Seen so far: 1952 samples\n",
            "****TROVATA LA FINE*****\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}